{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning\n",
        "\n",
        "딥러닝은 모형들의 output이 다시 input으로 들어가서 학습을 이어나가는 Layer 구조를 가지는 모형. 모형의 특징을 결정하는데는 다음과 같은 요소들이 필요\n",
        "\n",
        "\n",
        "\n",
        "*   모델 Building\n",
        "    * Connectivity Patterns\n",
        "    * Nonlinearity Modules\n",
        "    * Loss function\n",
        "\n",
        "*   모델 학습\n",
        "    * Optimization\n",
        "    * Hyper Parameters\n",
        "\n",
        "# Connectivity Pattern\n",
        "\n",
        "딥러닝은 여러 레이어를 쌓아나가는 구조. 뉴런은 각 레이어에 있으며, 레이어들간의 연결관계에 따라서 패턴이 나뉨.\n",
        "\n",
        "* Fully-Connected\n",
        "* COnvolutional\n",
        "* Dilated\n",
        "* Recurrent\n",
        "* Skip / Residual\n",
        "* Random\n",
        "\n",
        "여기서는 간단한 Connectivity Pattern만 정리\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qh7qenGU8Z55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. FC Layer\n",
        "뉴럴넷의 가장 간단한 아키텍쳐. 필요한 것은 레이어의 갯수와 뉴런의 갯수 그리고, 각 layer들의 연결 패턴을 고려해야함. 이번 케이스에서는 input, hidden, output layer를 각각 하나씩 가지고 있는 모듈을 만듬. connectivity pattern은 fully connected로 함.\n",
        "* Layer number\n",
        "* Neuron number\n",
        "* connectivity pattern"
      ],
      "metadata": {
        "id": "q73eZR9-9b-N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PHYCi4kN2Hag"
      },
      "outputs": [],
      "source": [
        "layer_dims = [5, 4, 3, 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0, Z)\n",
        "    assert(A.shape == Z.shape)\n",
        "    cache = Z\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "O0QoABDL-Z5m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    # dict 객체 생성\n",
        "    parameters = {}\n",
        "    # 총 layer들의 길이 계산\n",
        "    L = len(layer_dims)\n",
        "    # 레이어들을 돌면서, 레이어들 간의 weight와 bias의 초기값의 난수 생성\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) # *0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "        # assert를 통해 dimension 맞춤. 틀릴시 error 발생\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "GNWdKqjH-t_5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = initialize_parameters_deep(layer_dims)\n",
        "parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvCVKCe___Va",
        "outputId": "ae550a89-10a3-40bc-dfb1-b0d811fd1cdc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.48801736,  0.36878669, -0.2937001 ,  0.57356559,  0.25579563],\n",
              "        [-0.89222788,  0.15173943,  0.37397418,  0.04629986,  0.72582663],\n",
              "        [-0.01686243, -0.36319177, -0.15185667,  0.63758029,  0.93262974],\n",
              "        [-0.1446226 ,  0.17641405, -0.2385277 ,  0.46009671,  0.09729178]]),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'W2': array([[-0.23618477, -0.49271991, -0.55171079, -0.47334635],\n",
              "        [-0.16752299,  0.35649675, -0.05892275,  0.39333935],\n",
              "        [-0.429569  ,  0.08679653, -0.40196401, -0.41667964]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'W3': array([[-0.26504679,  0.35880478,  0.11373019]]),\n",
              " 'b3': array([[0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters['W1'].shape # param shape 개같이 구성해놓은듯"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om-yuWQTAFDo",
        "outputId": "59873b62-e97e-4e7d-f11b-849ebbecc254"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    # W에 A를 내적. 그 후 b를 더해줌.\n",
        "    Z = W.dot(A) + b\n",
        "    # Z의 shape이 input과 weigth의 shape과 동일한지 체크.\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    # 계산단계에서 사용한 값을 cache에 저장\n",
        "    cache = (A, W, b)\n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    # Activation func의 종류에 따라서 값을 나눔.\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    # Shape이 input과 weight와 동일한지 체크.\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    # linear 연산과 activation 연산을 cache에 저장.\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "pT3ki9uEBHUD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    # cache 들의 list.\n",
        "    caches = []\n",
        "    A = X\n",
        "    # weight와 bias가 저장되어 있기 때문에 //2 를 해줘야 layer의 사이즈가 됨.\n",
        "    L = len(parameters) // 2  # layer 개수\n",
        "\n",
        "    # hidden layers relu 통과\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    # output layer는 sigmoid를 통과하게 함.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    assert(AL.shape == (1, X.shape[1]))\n",
        "    return AL, caches"
      ],
      "metadata": {
        "id": "faN5AICTC-zU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(5, 4)\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "AL, caches = L_model_forward(X, parameters)"
      ],
      "metadata": {
        "id": "hs0BKZTUEfrb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8VzRwSqFG4E",
        "outputId": "be805c82-baef-4fa7-b9f9-ed55dc303ae0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.55217926, 0.55484082, 0.5       , 0.56343183]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Cost Function\n",
        "\n",
        "우리는 신경망을 통과한 $\\hat{y}$값을 찾을 수 있었음. 하지만 우리의 실제 y 레이블과는 다른 값일 가능성이 매우 크기 때문에 이를 반영하여 학습을 시켜야함. Cost function은 여러가지 종류가 있지만, 이번의 경우에는 cross-entropy 함수를 사용하려고함. 이후에 Cost function에 대해서도 정리할 예정.\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
        "\n",
        "### Arguments\n",
        "\n",
        "AL : 뉴럴넷을 통과해서 나오게된 $\\hat{y}$. shape (1, # of examples)<br/>\n",
        "Y : 실제 \"label\" vector. shape (1, # of examples)\n",
        "\n",
        "### Returns\n",
        "\n",
        "cost : cross-entropy cost"
      ],
      "metadata": {
        "id": "wtl-8LC_FUWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = (-1.0/m)*np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y, np.log(1-AL)))\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "_8lomhrXFHb6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost = compute_cost(AL, Y)\n",
        "print(\"cost = \" + str(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmPcWCWcHCNB",
        "outputId": "c6f7f70c-a80e-49ed-f3fa-50fbdf471344"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 0.7285985463399304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation Process\n",
        "Backpropagation은 다음과 같은 process를 가지게 됨.\n",
        "* Linear backward\n",
        "* Linear -> Activation backward\n",
        "* Layer -> Layer backward"
      ],
      "metadata": {
        "id": "ZbVymTV6JRGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear backward\n",
        "\n",
        "Linear 한 영역에서 backward 과정은 다음과 같은 인자를 받게됨.\n",
        "\n",
        "### Arguments\n",
        "dZ : Z의 변화량. linear 부분에서 output이 cost function에 대한 gradient를 나타냄.<br/>\n",
        "cache : forward과정에서 필요한 값을 받아옴. tuple 형태의 (A_prev, W, b) 값들을 받아옴.\n",
        "\n",
        "### Returns\n",
        "dA_prev : Linear 구간의 input으로 들어왔었던, 지난 레이어의 activation을 통과한 A가 cost func에 대한 변화량.<br/>\n",
        "dW : Linear 구간의 weight의 cost func에 대한 변화량.<br/>\n",
        "db : Linear 구간의 bias의 cost func에 대한 변화량\n",
        "\n",
        "## Linear-Activation backward\n",
        "\n",
        "Activation func g(.)에 대해서 Linear-activate backward는 다음과 같이 계산됨.\n",
        "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
        "\n",
        "### Arguments\n",
        "\n",
        "dA : 현재 layer의 gradient값이 인자로 들어옵니다.<br/>\n",
        "cache : forward pass에서 계산했던 linear(Z) 부분과 activation(A) 부분의 계산값들을 받음.\n",
        "\n",
        "### Returns\n",
        "\n",
        "dA_prev : Linear 구간의 input으로 들어왔었던, 지난 레이어의 activation을 통과한 A가 cost func에 대한 변화량.<br/>\n",
        "dW : Linear 구간의 weight의 cost func에 대한 변화량.<br/>\n",
        "db : Linear 구간의 bias의 cost func에 대한 변화량\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
        "\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
        "\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
      ],
      "metadata": {
        "id": "nKg4U_7iJibR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    Z = cache\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "VoRxD8WaHJG6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    dW = np.dot(dZ, cache[0].T) / m\n",
        "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(cache[1].T, dZ)\n",
        "\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "kLwULwu0MLa6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    grads = {} # 빈 dict 호출\n",
        "    L = len(caches) # layer의 갯수를 caches로 부터 받아옴.\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # Shape을 AL과 동일하게 해줌.\n",
        "\n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
        "    # caches index를 잡아둠.\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
        "\n",
        "    for l in reversed(range(L-1)):\n",
        "        # indexing\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, activation=\"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l+1)] = dW_temp\n",
        "        grads[\"db\" + str(l+1)] = db_temp\n",
        "    return grads"
      ],
      "metadata": {
        "id": "X1iq3pgfNcdZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads = L_model_backward(AL, Y, caches)\n",
        "grads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXB_Xk6sP9DX",
        "outputId": "04002d97-f83e-48a5-ddec-9aa48e9045c7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dA2': array([[-0.14635334,  0.11798801,  0.13252339, -0.14933579],\n",
              "        [ 0.19812456, -0.15972524, -0.17940239,  0.20216203],\n",
              "        [ 0.06279945, -0.05062804, -0.0568651 ,  0.06407921]]),\n",
              " 'dW3': array([[0.        , 0.10525171, 0.02263592]]),\n",
              " 'db3': array([[0.04261298]]),\n",
              " 'dA1': array([[-0.03319042,  0.02675765,  0.        , -0.06139323],\n",
              "        [ 0.07063076, -0.05694153,  0.        ,  0.07763196],\n",
              "        [-0.01167404,  0.00941145,  0.        , -0.03766948],\n",
              "        [ 0.07793019, -0.06282622,  0.        ,  0.05281778]]),\n",
              " 'dW2': array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "        [ 0.03456726,  0.10784149, -0.0304345 ,  0.00843343],\n",
              "        [ 0.        ,  0.02966002,  0.        ,  0.        ]]),\n",
              " 'db2': array([[0.        ],\n",
              "        [0.06014034],\n",
              "        [0.0160198 ]]),\n",
              " 'dA0': array([[-0.05789485,  0.03758792,  0.        , -0.0692654 ],\n",
              "        [ 0.01646518, -0.00219057,  0.        ,  0.01177983],\n",
              "        [ 0.01934638, -0.03058258,  0.        ,  0.02903235],\n",
              "        [ 0.01264559,  0.01871144,  0.        ,  0.00359435],\n",
              "        [ 0.03947013, -0.02570779,  0.        ,  0.05634734]]),\n",
              " 'dW1': array([[ 0.01403478,  0.0032155 ,  0.01168147,  0.00259168,  0.00710302],\n",
              "        [-0.06758436, -0.02356492, -0.03207737, -0.03681622, -0.00276196],\n",
              "        [ 0.00493644,  0.00113098,  0.00410871,  0.00091157,  0.00249834],\n",
              "        [-0.0300999 ,  0.00943109,  0.00308413, -0.00627989,  0.00951062]]),\n",
              " 'db1': array([[-0.00160819],\n",
              "        [ 0.0228303 ],\n",
              "        [-0.00056565],\n",
              "        [ 0.01948255]])}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Update parameter\n",
        "\n",
        "파라미터를 업데이트 하는 규칙은 생각보다 간편함. Learning rate인 ${α}$에 Gradient를 곱해서 현재의 param에 빼주면 새로운 param이 됨.\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "\n",
        "### Arguments\n",
        "\n",
        "parameters : 파라미터들이 담겨져 있는 parameter dict.<br/> grads : Gradient들이이 담겨있는는 dict.\n",
        "\n",
        "### Returns\n",
        "\n",
        "parameters :  업데이트 되어있는 파라미터들이 담긴 dictionary\n",
        "* parameters[\"W\" + str(l)] = ...\n",
        "* parameters[\"b\" + str(l)] = ..."
      ],
      "metadata": {
        "id": "aO53AA3aQG4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2 # layer 갯수\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "kDXSVbNtQBHU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = update_parameters(parameters, grads, 0.05)\n",
        "parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llP-ExhaYAds",
        "outputId": "69c471a9-fad1-466d-b69b-cccd2c1f2d73"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.4887191 ,  0.36862591, -0.29428418,  0.573436  ,  0.25544048],\n",
              "        [-0.88884866,  0.15291767,  0.37557805,  0.04814068,  0.72596473],\n",
              "        [-0.01710925, -0.36324832, -0.1520621 ,  0.63753471,  0.93250482],\n",
              "        [-0.1431176 ,  0.1759425 , -0.23868191,  0.46041071,  0.09681625]]),\n",
              " 'b1': array([[ 8.04096028e-05],\n",
              "        [-1.14151490e-03],\n",
              "        [ 2.82824182e-05],\n",
              "        [-9.74127316e-04]]),\n",
              " 'W2': array([[-0.23618477, -0.49271991, -0.55171079, -0.47334635],\n",
              "        [-0.16925136,  0.35110468, -0.05740103,  0.39291768],\n",
              "        [-0.429569  ,  0.08531353, -0.40196401, -0.41667964]]),\n",
              " 'b2': array([[ 0.        ],\n",
              "        [-0.00300702],\n",
              "        [-0.00080099]]),\n",
              " 'W3': array([[-0.26504679,  0.35354219,  0.1125984 ]]),\n",
              " 'b3': array([[-0.00213065]])}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgZmEqA4YHbe",
        "outputId": "820ae512-16a6-486a-bc26-ef4f886efcfd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.48801736,  0.36878669, -0.2937001 ,  0.57356559,  0.25579563],\n",
              "        [-0.89222788,  0.15173943,  0.37397418,  0.04629986,  0.72582663],\n",
              "        [-0.01686243, -0.36319177, -0.15185667,  0.63758029,  0.93262974],\n",
              "        [-0.1446226 ,  0.17641405, -0.2385277 ,  0.46009671,  0.09729178]]),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'W2': array([[-0.23618477, -0.49271991, -0.55171079, -0.47334635],\n",
              "        [-0.16752299,  0.35649675, -0.05892275,  0.39333935],\n",
              "        [-0.429569  ,  0.08679653, -0.40196401, -0.41667964]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'W3': array([[-0.26504679,  0.35880478,  0.11373019]]),\n",
              " 'b3': array([[0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z--xN544YS6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}